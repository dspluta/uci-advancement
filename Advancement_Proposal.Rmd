---
title: "Statistical Methods for Connectome Genetics"
subtitle: "Research Proposal for Advancement to Candidacy"
author: "Dustin Pluta"
date: "March 28th, 2018"
output: bookdown::pdf_document2
bibliography: main.bib
nocite: | 
  @*
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Research Objectives

<!-- Neuroimaging technologies have given scientists the means to study the relationship of _in vivo_ brain activity and the resultant cognition and behavior in unprecedented detail.  These technologies are being constantly improved and refined, leading to studies involving more subjects and a wide variety of measured features from multiple data sources, which has in turn required more sophisticated statistical methods to make efficient and effective use of the data.  Despite significant progress in cognitive neuroscience and neurophysiology, there remain many unknowns regarding the mechanisms and factors of brain function. -->

<!-- A common goal of modern scientific studies is determining statistically significant relationships between multiple sets of features. In a collaborative work with behavioral scientists, the key interest of our research is to determine the extent to which genetic factors explain the variability in brain function, with a focus on brain connectivity, and how both genetics and brain function influence human behavior. In most biomedical studies, these sets of features are complexly structured, and are often orders of magnitude larger than the sample size.  As a consequence of the ``curse of dimensionality'' in high-dimensional settings, only individual factors with large effect sizes can be detected after adjusting for multiple comparisons. The deficiencies of massive univariate analyses have motivated methods that aggregate effects from many individual variables. The goal of such methods is to assess the joint effect of a set of variables, rather than separately estimating many univariate effects, on the outcome of interest. Examining the overall effects is particularly relevant in the presence of two sets of high-dimensional data from distinct measurement modalities. -->

In recent years, the set of the dynamic and structural relationships between brain regions, referred to as the *connectome*, has been shown to play a crucial role in cognition and behavior.  Advances in neuroimaging technologies have facilitated a more thorough and detailed study of the connectome at all scales, while developments in neurophysiology and cognitive psychology have motivated new theories and new questions regarding the structure of the connectome.  Concurrent with the development of connectomics, the field of imaging genetics has emerged as an important area of research for deepening our understanding of the many factors that influence cognitive functions.  Given the extreme complexity of both the brain and the genome, there is a pressing need for more refined and powerful statistical methods tailored to imaging genetics data.

While a number of statistical methods exist for analyzing imaging genetics data, these methods suffer from a number of problems.  Many of the models employed are high-dimensional extensions of previously existing approaches, and so may not be efficient in answering the questions of interest for imaging genetics studies.  Conversely, the more novel methods motivated specifically by imaging genetics data tend to be complicated and computationally demanding, and still often are not easily interprettable in terms of the most relevant scientific concepts.  

We here propose the development of a collection of statistical methods and models for the analysis of imaging genetics data, with emphasis on applications to connectome genetics.  Specifically, the proposed methods will allow one to test the significance of association for genetic factors and neuroimaging phenotypes, and for the estimation of differences in connectivity with respect to variations in genotypes across individuals.  The estimates and inferences of these methods will yield scientifically interpretable and relevant insights regarding the role of particular genetic influences on the human connectome, and may lead to a greater understanding of the underlying factors that determine complex cognitive traits such as learning styles, memory performance, and decision-making.  Moreover, the proposed methods are designed to accommodate a wide variety of high-dimensional data types, and are suitable for many applications beyond imaging genetics.

_Aim 1._ To develop __flexible global association testing methods__ suitable for inference with high-dimenesional multi-modal data produced by current neuroimaging studies.  These methods will provide a means to quickly explore possible significant relationships in these high-dimensional data sets as part of a preliminary analysis.  While many such methods currently exist, a rigorous methodology for effectively applying these methods in high-dimensional settings is still needed, and moreover, current methods are often too underpowered for practical use.  Our preliminary work suggests that adapting penalized kernels methods for association testing may yield much more powerful tests compared to existing approaches. The developed methods will target applications to imaging genetics studies, with a focus on meaningful interpretation of results in terms of standard concepts in the genetics literature, such as phenotype heritability.

_Aim 2._ To develop __manifold regression models for heritability analysis of connectivity__.  Building on the methods and challenges in Aim 1, a mixed-effects model for heritability analysis of manifold-valued phenotypes will be developed to increase the power and efficiency in testing and estimating the effects of genetics on high-dimensional phenotypes, such as brain connectivity measurements.  By utilizing the inherent structure of connectivity matrices and other high-dimensional phenotypes measured from neuroimaging data, we will develop more natural and interpretable methods to reduce dimensionality when modeling genetic effects on these phenotypes.  A practical mixed-effects model for heritability analysis of manifold-valued data would have important applications for many types of neuroimaging and connectomic data.

_Aim 3._ To develop __multi-subject dynamic connectivity models__ for fMRI and EEG data.  Many approaches to modeling dynamic connectivity have been proposed in recent years, but the majority of these models are not able to jointly model multiple subjects, and many require unreasonable assumptions regarding the scale and structure of relevant features. The developed models will extend the switching factor VAR model to multiple subjects, which will allow for the estimation of recurring connectivity states that are common across subjects, without requiring the specification of a time scale or the temporal alignment of subject data.  The current switching factor VAR models will be enhanced with the methods developed in Aims 1 and 2 in order to identify common connectivity subspaces across subjects to facilitate multi-subject modeling.  Motivated by the scientific goal of understanding the genetic influences on dynamic connectivity features, these models will be further developed to allow for covariate adjustment, and the estimation and interpretation of covariate effects.  In particular, the developed models will be applied to assess the influence of genetics on various features of dynamic connectivity as measured by fMRI or EEG across a variety of experimental settings.

_Aim 4._ __For broad impact on science:__ The above developed models will be applied to thoroughly analyze imaging genetics data from a study of 1000 healthy Chinese college students, collected by collaborators at Beijing Normal University.  The data includes measures from a variety of psychological tasks related to learning and memory, and EEG, fMRI, and DTI imaging, as well as genetic information in the form of approximately $5 \times 10^5$ SNPs.  The goals of the study include the identification of genetic factors that influence both neurological activity and behavioral outcomes.  We will also apply our methods to analyze data from the Human Connectome Project (HCP), which includes a variety of fMRI scans of both related and unrelated subjects (including SNP data for unrelated subjects).  The HCP data is open access, which will allow us to directly compare our results to other analyses of the same data, and from which we will provide a publicly available and entirely reproducible analysis using our developed methods.  Code for the applied methods will also be made available in a set of `R` packages.

# Background

## Measuring Brain Connectivity

Commonly used technologies in current neuroimaging studies include structural and functional magnetic resonance imaging (MRI and fMRI), electroencephalogram (EEG), magnetoencephalogram (MEG), and diffusion tensor imaging (DTI).  Since each of these imaging modalities measures a different set of neural features or phenomena, they may differ in their effectiveness in answering particular scientific questions.  For instance, fMRI measures changes in the blood oxygen level over time in over $5\times 10^5$ voxels throughout the brain (@lindquist2008statistical), with very high spatial resolution (typically less than 3mm), but poor temporal resolution (around 2s).  In contrast, EEG measures the collective cortical activity of populations of neurons via 64-256 electrodes placed on the scalp (@Ombaobook), allowing for a high temporal resolution (1 kHz) at the cost of spatial resolution.  Instead of measuring dyanimc brain activity, strucutral imaging methods provide information about the physical relationship of brain regions, such as DTI, which measures the structure and orientation of the brain's white matter fiber tracks via the diffusion of water molecules through the entire brain volume (@basser1994estimation).

It is now generally believed that higher-level cognitive processing (e.g., in memory retrieval, decision making) critcally depends on the interaction and transfer of information between many localized regions. Numerous different methods for calculating brain connectivity have been proposed, but the reliability, interpretation, and relationship of these different measures is not well established (@fiecas2013quantifying).  To date, there are three concepts of brain ``connectivity" that have been of primary interest, namely \textit{structural}, \textit{functional}, and \textit{effective} connectivities. Structural connectivity refers to the anatomical connections between brain regions, measured using DTI or structural MRI. Functional connectivity is a symmetric and undirected measure of concordant activity between brain regions, commonly calculated as Pearson's correlation, partial correlation, coherence, or partial coherence between the activation signals of two regions (@FiecasPCoh).  By contrast, effective connectivity is a directed measure of how past activity in one region may influence the future activity of another region. Effective connectivity is closely related to Granger causality and is often estimated with a vector autoregressive (VAR) model (@Gorrostieta2012, @Gorrostieta2013, @BayesianVAR). Modalities used for functional and effective connectivity studies include fMRI, EEG, and MEG.  Despite the continued use of functional and effective connectivity measures in neuroimaging studies over the past two decades, there remain open questions regarding the rigor and underlying statistical assumptions of these methods.

```{r, echo = FALSE, fig.width = 5, fig.height = 6, fig.align='center'}
img <- png::readPNG("img/Coh-Illustration2.png")
grid::grid.raster(img)
```

<!-- Illustration of calculating coherence between two time series $Z_1$ and $Z_2$.  The coherence between these time series at a frequency $\omega$ measures the correlation of amplitudes at $\omega$. -->

In studying the etiology of neurological diseases such as Alzheimer's, schizophrenia, and autism spectrum disorders, significant links with these forms of brain connectivity have been consistently shown (@woodward2015resting).  There is also evidence that dynamic characteristics of brain connectivity, and not just global "static" measures of connectivity over an entire experiment, are also important for understanding brain function.  Formally defining and modeling dynamic connectivity has motivated the development of new statistical methods.  Current approaches for modeling dynamic connectivity include sliding window methods (e.g., @chang2010time), graphical Bayesian modeling (@warnick2017bayesian, allen2014tracking, zalesky2014time, lindquist2014evaluating, havlicek2010dynamic), change-point detection via VAR models (@KMO), and hidden Markov switching-VAR models (@samdin2017unified).  Our proposed research focuses on extensions of the regime switching factor model (sf-VAR) proposed in @ting2017estimating.  A brief description of a three-step procedure for fitting this model is as follows.  (1) Compute initial estimates of connectivity subspaces shared across regimes using a stationary factor model; (2) apply a factor SVAR model, and identify regime boundaries with a switching Kalman filter to partition the neural signal into a small number of distinct states; (3) finally, use the low-dimensional factor representations for each regime to estimate within-regime effective connectivity. This model is able to detect abrupt changes in mental state, such as might occur in an experiment with varied cognitive demands, and is also able to estimate recurring states over the course of the experiment.  However, this model does not currently allow for estimation of common connectivity states across subjects, thus limiting the scientific conclusions that can be drawn from the analysis.

<!-- ![SF-VAR for Dynamic Connectivity (@samdin2017unified)](img/SF-VAR_original_schematic.png) -->

## Models and Tests for Imaging Genetics

Human traits, including neural, cognitive, and behavioral traits, are shaped by genetics and
environmental factors through a number of complex biological processes. In the past few
decades, single nucleotide polymosphisms (SNPs) have become one of the most commonly
measured forms of genetic data, as they are abundant (about 10 million in the human
genome), stable, and easy to measure (@us_library_medicine_SNPs).  Each SNP measurement 
contains the allele at a particular base pair location in a subject's DNA, 
typically coded as a 0 for the common homozygotic allele, 1 for the heterozygotic allele, 
and 2 for the uncommon homozygotic allele. Many complex traits are believed to 
depend on higher-order interactions between collections of SNPs, but
determining these relationships is difficult, and in practice it is common to
restrict attention to only the additive effects of SNPs. The proportion of variance of an
observed trait that is accounted for by the additive effect of genome-wide SNPs is referred
to as narrow-sense heritability, denoted $h^2$.

For many cognitive phenotypes, there is currently little prior information on heritability
estimates, important genetic factors, and functional relationships. Given this lack of
knowledge regarding the structure and role of genetic influences, global association
testing is a convenient method to evaluate the strength and significance of genetic
influences on a particular observed trait. In genome-wide association studies (GWAS) using unrelated subjects, the
random effects (or variance components) model is a standard tool to estimate the heritability of complex traits (@goeman2006testing; @liu2007semiparametric; @boyle2017expanded), 
popularized in recent years by the freely available Genome-wide Complex Trait Analysis (GCTA) software (@yang2011gcta). 
In the variance components model, the effect of an individual genetic variant is treated as a random quantity, rather
than a fixed parameter, so that the genetic effects of a large number of SNPs can be parameterized by a small number of variance components. As a result of this, the number of genetic
variants studied is not required to be less than the sample size. This method is closely
related to semi-parametric kernel regression methods, which have received increasing attention 
in this area as a means to flexibly model non-additive effects of SNPs (@schaid2010genomic). The variance components model and related kernel methods have also been extended to model the heritability of multi-dimensional phenotypes, such as brain activation or shape (@ge2016multidimensional).

The variance components model commonly used for genetic analysis is a linear random effects model, from which one can estimate the narrow-sense (or additive) heritability of a phenotype, denoted $h^2$.  Formally, let $Y$ be an $n \times 1$ vector of observed continuous phenotype measurements from $n$ subjects, and $X$ be the $n \times p$ SNP data matrix for $p$ SNPs.  The heritability model is

$$Y = Xu + \varepsilon,$$

where $u$ is a $p \times 1$ vector of random effects with $u \sim \mathcal{N}_p(0, \sigma^2_u \otimes I_p)$, and $\varepsilon \sim \mathcal{N}_n(0, \sigma^2_{\varepsilon} \otimes I_n)$.  Since the target of inference 
is the total genetic contribution, this model is often rewritten by aggregating the individual SNP effects $Xu = g$, yielding

$$Y = g + \varepsilon,$$

where $g \sim \mathcal{N}(0, \sigma^2_g \otimes G)$, with $G = XX^T/p$, referred to as the _genetic relationship matrix_.  Estimates of the variance components of the model $\hat \sigma^2_g, \hat\sigma^2_{\varepsilon}$ can be computed using standard methods for mixed effects models, with the narrow-sense heritability calculated as

$$\hat h^2 = \frac{\hat\sigma^2_g}{\hat\sigma^2_g + \hat\sigma^2_{\varepsilon}}.$$

Distance-based testing methods, which include the Mantel test (@mantel1967), RV coefficient (@robert1976unifying), and the distance-covariance test (@szekely2007measuring), are a common class of methods to test for global association between two sets of features using the pairwise distance or similarity between observations in each
feature set. These methods can easily accommodate data of arbitrary dimension, and are
generally straightforward to implement and interpret. In our recent work, we have found that
Mantel’s test (using the $L_2$ inner product to measure similarity between subject) is equivalent to the global
score test for $h^2$ from the random effects model (@dustin2017mantel). By adopting other
similarity measures, such as weighted inner products or non-linear kernels, the Mantel test
can also be used to conduct global association tests corresponding to other methods, 
e.g. multivariate distance matrix regression, pseudo F-tests,
and kernel-based tests. The flexibility of this approach makes it possible to test a wide variety
of relationships and data types in a common framework. For details on generalizations and
applications of distance-based methods, see @schaid2010genomic; @pan2011relationship; @zapala2012statistical; @ge2016multidimensional; @dustin2017mantel.

Although distance-based methods have many attractive features, there is evidence that they
can be severely underpowered in high-dimensional settings, making them impractical for 
many applications of interest. Alternative methods, such
as the “sum of powered correlation” (SPC) and “sum of powered score” (SPU) tests, have
been proposed as more powerful for imaging genetics data. Versions of these tests have been shown to perform better than distance-based methods in some simulations (@xu2017adaptive).

In @dustin2017mantel, we have proposed the adaptive Mantel test as an extension of the
classical Mantel test (or the RV coefficient test) that utilizes penalized regularization
as in ridge regression in order to achieve higher power in high-dimensional settings 
compared to the other association testing methods mentioned above.  By following the 
"adaptive" algorithm from @xu2017adaptive, the adaptive Mantel test is able to 
simultaneously test a set of candidate penalization terms while still maintaining 
the correct type I error, and without suffering a severe loss of power relative to 
a univariate Mantel test, thus making it a practical alternative for association 
testing in imaging genetics.

Inferential modeling for imaging genetics is challenging, in part due to the extremely high-dimension of the feature spaces. The most commonly used data-driven methods in these settings are principal component analysis (PCA), and independent component analysis (ICA) (@liu2009combining). 
Although these methods have proven effective for many applications, they have the drawback that extracted leading components do not necessarily capture the most significant associations. In contrast, simultaneous dimension reduction methods, such as partial least squares (PLS), canonical correlation analysis (CCA), reduced rank 
regression (RRR), and parallel ICA (@ahn2015sparse), perform dimension reduction jointly
on responses and covariates, and may be preferred for analyzing imaging genetics data. Regularization methods developed from ridge regression and the LASSO have been widely used in these contexts as well (@tibshirani1996regression).

Tensor regression is a more recent approach for modeling multi-way array data, which utilizes a
low-rank tensor decomposition (such as PARAFAC) to reduce dimensionality (@zhou2013tensor); 
@Guhaniyogi2016bayesiantensor). This model has a number of advantges. Because
the low-rank representation is estimated from the model, the extracted features are more
likely to retain significant relationships in the data. Furthermore, the tensor decomposition
can directly incorporate some of the inherent structure in the data, such as the spatial
organization in neuroimaging data. This is an important distinction from the commonly
used PCA or PLS, which are similar dimension reduction methods, but which require the imaging
data to be vectorized, thereby destroying the natural array structure. Incorporating regularization
methods, such as the LASSO, is also straightforward. Tensor regression is potentially well-
suited to modeling connectivity, which is naturally written as a matrix, or possibly even
dynamic connectivity, although these applications of tensor regression have not yet been
thoroughly studied.

In a similar vein, manifold regression methods seek to use the 
inherent structure of the data to improve modeling efficiency.  For instance, 
covariance matrices are (often) positive definite, and so belong to the 
Riemannian manifold of positive definite matrices.  This is directly 
relevant to modeling many types of brain connectivity, although the 
development of these models is still in its early stages, and 
many theoretical and computational challenges remain.  In previous work, manifold-value regression 
models have been proposed as analogs to the fixed effects (@zhu2009intrinsic) and 
mixed effects models (@kim2017riemannian).  Applications to DTI data show some promise with these methods, although they can be technically challenging to implement.

## Review of Previous Results in Imaging Genetics

Some hallmark successes in genetics have come from the study of prominent neurological and
psychological diseases, such as Parkinson’s disease, Alzheimer’s disease, and schizophrenia.
For each of these diseases, many significant genetic risk factors of relatively large effect have
been found, which have in turn led to improved understanding of the mechanisms of action
for these diseases, and have even resulted in promising gene therapies for Parkinson’s disease
(Palfi et al. (2014)). Initially motivated by these promising results, the field of imaging ge-
netics has expanded to now consider a number of different neural disorders and many facets
of cognition in healthy-individuals. We mention a few recent results to illustrate the breadth
of current research. The study by Bohlken et al. (2016) found that global reductions in
white matter, which underpin structural connectivity, are largely explained by genetic risk
factors for schizophrenia. In very recent work by Sudre et al. (2017), a study of families
with histories of attention-deficit/hyperactivity disorder (ADHD) found significant genetic
heritability for the default mode, cognitive control, and ventral attention networks. An
analysis of 161 twin pairs found significant heritability of anatomical connectivity measured
with DTI, and identified significant differences in heritability across subnetworks (with av-
erage heritability over all white matter tracts estimated at around 30% (Shen et al. (2014)).
With data from 1320 unrelated, young, healthy adults, Ge et al. (2016) demonstrated that
several characteristics of brain structure are genetically heritable, such as brain volume and
neuroanatomical shape. Using resting-state fMRI data from twins and non-twin siblings,
Vidaurre et al. (2017) found evidence of significant heritability for certain characteristics of
dynamic connectivity patterns; this is one of the few studies to date to have considered the
heritability of dynamic connectivity.


# Proposed Research

## Aim 1: Development of the Adaptive Mantel Test

# Preliminary Results

## Materials and Methods

Our collaborators at the Beijing Normal University (BNU) Center for Brain and Learning Sciences have conducted a large imaging genetics study, which collected data from approximately 1000 healthy college Chinese students participated. The study participants were given a number of standard psychological tasks to measure cognitive traits such as working memory performance, learning rates, and risk-taking behavior.  During each of these tasks, fMRI or EEG data was recorded.  Structural MRI, DTI, and SNP data was also collected from the subjects.

For our preliminary analysis, we have considered data from 350 BNU subjects who participated in a visual working memory task, during which 64-channel EEG was recorded at 1 kHz.  The total duration of the experiment was 5-10 minutes for each subject.  Approximately $5 \times 10^5$ SNPs were also measured for each subject. Standard pre-processing and quality control steps were applied to both the EEG and genetic data.  The goal of the analysis is to determine the association of theta, alpha, and beta band coherence with a group of 13 SNPs that have been identified as potentially related to Alzheimer's Disease.  

This results in 2080 distinct features when using all 64 channels, and 300 distinct features for the 25 selected frontal channels.  The adaptive Mantel test was performed with $\lambda \in \{0.5, 1, 5, 10, 100, 1000, \infty\}$ and using 1000 permutations.  Genetic similarity of subjects was calculated as the $L_2$ inner product of the centered standardized SNP data for all tests.

The coherence between two EEG channels at a particular frequency $\omega$ is a measure of the oscillatory concordance of the the two signals at $\omega$.  The pairwise coherence for $q$ EEG channels is a $q \times q$ symmetric matrix, from which we extract the upper triangle and vectorize to form the $n \times {{q}\choose{2}}$ matrix $X$.  This results in 2080 distinct features when using all 64 channels, and 300 distinct features for the 25 selected frontal channels.  The adaptive Mantel test was performed with $\lambda \in \{0.5, 1, 5, 10, 100, 1000, \infty\}$ and using 1000 permutations.  Genetic similarity of subjects was calculated as the $L_2$ inner product of the centered standardized SNP data for all tests.  For the alpha band, all 64 channels gave $P = 0.065$, and the selected frontal channels gave $P = 0.381$.  Results for the theta band were $P = 0.416$ and $P = 0.085$ for all 64 channels and frontal channels respectively.  Since the adaptive Mantel test was used, these $P$-values already take into account testing across multiple $\lambda$. In the case of the alpha band, the test results suggest that coherence involving channels outside of the selected frontal channels may be associated with genetic similarity determined by the 11 AD SNPs, whereas for the theta band, the SNP association appears stronger when considering only the frontal channels.  For a better sense of the significance of these 11 SNPs, the AMT $P$-values for 200 sets of 11 randomly selected SNPs were calculated for each of the four tests considered here.  The boxplot of these values are given in Figure \@ref(fig:fig2).  For the alpha -- all channels test and the theta -- frontal channels test, the $P$-values from the AD SNPs (0.065 and 0.085 respectively) are outside the ranges of $P$-values from the randomly selected SNPs.

## Preliminary Results from Simulations

Preliminary simulation results have indicated the feasibility of the Adaptive Mantel as an alternative to multiple comparison adjustments for a collection of penalized Mantel tests in terms of statistical power for the simulation settings considered.  Moreover, relative to competing methods, including the classical distance Mantel test, RV-coefficient and distance-covariance tests, and the adaptive sum of powered score tests, the Adaptive Mantel test has sufficiently high power to make it a practical and easily implemented alternative.  

```{r, echo = FALSE, fig.width = 6, fig.height = 8, fig.align='center'}
img <- png::readPNG("img/simulation_sparsity_plot.png")
grid::grid.raster(img)
```

## Preliminary Results from BNU Data: Association of EEG Coherence and Selected SNPs

For the alpha band, all 64 channels gave $P = 0.065$, and the selected frontal channels gave $P = 0.381$.  Results for the theta band were $P = 0.416$ and $P = 0.085$ for all 64 channels and frontal channels respectively.  Since the adaptive Mantel test was used, these $P$-values already take into account testing across multiple $\lambda$. In the case of the alpha band, the test results suggest that coherence involving channels outside of the selected frontal channels may be associated with genetic similarity determined by the 11 AD SNPs, whereas for the theta band, the SNP association appears stronger when considering only the frontal channels.  For a better sense of the significance of these 11 SNPs, the AMT $P$-values for 200 sets of 11 randomly selected SNPs were calculated for each of the four tests considered here.  The boxplot of these values are given in Figure \@ref(fig:fig2).  For the alpha -- all channels test and the theta -- frontal channels test, the $P$-values from the AD SNPs (0.065 and 0.085 respectively) are outside the ranges of $P$-values from the randomly selected SNPs.

\begin{table}
\centering
\begin{tabular}{ll}
\hline
Test & $P$-value (Best $\lambda$)\\\hline
$\alpha$, All Channels & 0.065 (5)\\
$\alpha$, Frontal Channels & 0.381 (1)\\
$\theta$, All Channels & 0.416 (0.5)\\
$\theta$, Frontal Channels & 0.085 (0.5)\\
\hline
\end{tabular}
\caption{Testing results for associations of EEG coherence and AD SNPs.}
\end{table}

To assess possible contributions of individual channel pairs, each channel pair was separately tested for significance with the AD SNPs with the GCTA toolkit (@yang2011gcta).  Figure \@ref(fig:fig2) shows the most significant channel pairs across all channel pairs for the alpha band.  The TP8 -- CP2 connection is the most significant at $P < 0.0001$ (with no adjustment for multiple comparisons).  Other top pairs are C4 -- PO4, C4 -- AF7, and T8 -- FT7.  The most significant connections are mostly between the right temporal regions with the left frontal regions.  For the theta band, the most significant channel pairs were P9 -- Cz, P9 -- CP3, CP3 -- AF3, and P8 -- F3.  The P9 channel also had relatively significant connections with many other channels in the frontal left hemisphere.  These results are supported by a number of previous studies that have established links between working memory performance and features measured by EEG.  For instance, @onton2005frontal found increases in frontal midline theta power with increasing memory load during a verbal-working memory task;  @sauseng2005fronto also found that alpha coherence plays a significant role in ``top-down'' control during working memory tasks; and @simons2003prefrontal identified important interactions between the prefrontal and medial temporal lobes for the processing of long-term memory.

Similarly, the association of each individual SNP with EEG coherence was assessed with AMT.  For alpha coherence with all channels, the most significant SNP ($P = 0.02$) is rs2227564, a functional polymorphism within plasminogen activator urokinase (PLAU) gene.  An allele of this SNP has been linked to significantly high plaque counts in AD, although its role is not well-established.  The second most significant SNP from the individual tests is rs3851179, a SNP upstream of the PICALM gene.  This SNP has been repeatedly implicated as a factor in AD, as well as Parkinson's Disease and schizophrenia, although there are dissenting results regarding its significance in particular Chinese populations.  In the genetics literature on cognitive function in healthy subjects, polymorphisms in neurotransmitter genes, such as those in the dopamine pathway, have been shown to be significantly associated with increased neuronal activity in the prefrontal cortex during working memory tasks, as measured by fMRI @Bertolino3918.  Vogler et al. analyzed data from the $n$-back memory task for 2298 subjects, and estimated genome-wide heritability of working memory accuracy to be 41\% (95\% CI: 0.13, 0.69) @vogler2014substantial. 

#### Forms of Mantel's Test for Linear Association

If two statistics $T_1$ and $T_2$ produce equivalent test results when calculated on the same size $n$ sample, we refer to these statistics as \textit{testing equivalent}, and write $T_1 \asymp T_2$.  When the tests from $T_1$ and $T_2$ are asymptotically equivalent as $n \to \infty$, we say $T_1$ is \textit{asymptotically testing equivalent} to $T_2$, and write $T_1 ~\dot\asymp~ T_2$.  To develop the theoretical foundation of the adaptive Mantel test, we first discuss the original form of Mantel's test and introduce a modified form that is asymptotically testing equivalent. Consider \(n\) independent observations each measured on $\textbf{X} \times \textbf{Y}$, where $X$ is the $n \times p$ design matrix from the $p$-dimensional feature space $\bf X$ and $Y$ the $n \times 1$ response vector from a univariate feature space $\bf Y$, and further suppose $X$ is column-centered, and $Y$ is centered.  Let \(K\) be an \(n\times n\) similarity matrix that measures the similarity of observations in $\bf X$.  For $\delta^{\textbf{X}}(\cdot, \cdot)$ a similarity or dissimilarity metric on $\textbf{X} \times \textbf{X}$, let $K$ be the corresponding Gram matrix with $K_{ij} = \delta(X_i, X_j)$.  Similarily, for some metric $\delta^{\textbf{Y}}$ defined on $\textbf{Y} \times \textbf{Y}$, let \(H\) be the corresponding Gram matrix of $Y$. The Mantel test statistic is defined as

\[MT(X, Y) = \sum_{i = 1}^n \sum_{i<j} H_{ij}K_{ij}.\]

The reference distribution under the null hypothesis of
no association between the $\kappa^{\textbf{X}}$ measures and $\kappa^{\textbf{Y}}$ measures, can be obtained from the observed features $X$ and $Y$ by permuting the observation labels for one set of features and calculating the empirical reference distribution for $T$. Equivalently, one can hold one matrix fixed, say \(K\), and simultaneously permute the rows and columns of \(H\). In Mantel's original formulation, \(MT\) uses only the upper triangle of each matrix, excluding the diagonal. We here define a modified Mantel test statistic, denoted \(T\), as

\[T(X, Y) = \sum_i \sum_j H_{ij}K_{ij}=tr(HK).\]

Testing $T$ will not be equivalent to testing $MT$ when the diagonals of $H$ and $K$ are both nonconstant, but it can be shown that $T ~\dot\asymp~ MT$ (@martins2006note).  For the remainder of this article, we will focus on tests with $T$ calculated from similarity measures that can be written as weighted Euclidean inner products, as these tests have a close connection with linear models.  Extending these results to a broader class of kernel similarity measures is a direction for future work.

We now define three Gram matrices calculated from weighted inner products, which we denote $K_R, K_F,$ and $K_{\lambda}$ since these will be shown in the following section to correspond to the score tests for the random effects, fixed effects, and ridge regression models respectively. For a positive semi-definite (p.s.d.) weight matrix $\mathcal{W}$, the corresponding weighted inner product is calculated as $\langle X_i, X_j\rangle_{\mathcal{W}} = X_i^T \mathcal{W} X_j$.  Choosing $\mathcal{W} = \textbf{I}$ gives
the standard Euclidean inner product, with Gram matrix
$$K_R := XX^T$$
where \(K_{R, ij} = \langle X_i, X_j\rangle\), \(K_{R, ii} = \|X_i\|^2\).  Another natural choice for weight matrix is $\mathcal{W} = (X^TX)^{-1}$ (assuming the inverse exists), which is the projection matrix into $\mathcal{C}(X)$, the column space of $X$.  The Gram matrix is

\[K_F := X(X^TX)^{-1}X^T.\]

\noindent $K_F$ is also recognizable as the ``hat matrix'' from the fixed effects model, and is related to the Mahalanobis distance.  When $X^TX$ is not full rank, such as when $n > p$, we can replace $(X^TX)^{-1}$ with a generalized inverse $(X^TX)^{-}$, since the similarity matrix is invariant to the choice of inverse. Alternatively, we can pre-condition the weight matrix by adding a positive constant $\lambda$ to the diagonal, i.e. \(\mathcal{W} = (X^TX + \lambda I_p)^{-1}\).  This gives similarity matrix

\[K_{\lambda} := X(X^TX + \lambda I_p)^{-1}X^T.\]


#### Linear Model Score Tests

In general, the score test is defined as follows. For a stochastic model
with parameters \(\theta = (\beta, \alpha)\) and observations \((X, Y)\), and with likelihood \(\mathcal{L}(\theta)\), score vector
\(\mathcal{U}(\theta) = \frac{\partial \log \mathcal{L}}{\partial \theta}\), and Fisher
information
\(\mathcal{I}(\theta) = -\mathbb{E} \frac{\partial}{\partial \theta} \mathcal{U}(\theta)\), the
score test statistic for testing $H_0: \beta = 0$ is

\[S=[\mathcal{U}(\theta)^T (\mathcal{I}(\theta))^{-1}\mathcal{U}(\theta)]|_{\beta=0}.\]

The classical fixed effects model is robust and broadly applicable for simple association testing, but requires $n > p$, and so is not feasible for high-dimensional settings.  It does however serve as a useful theoretical comparison to the random effects and ridge regression models to be discussed next. The fixed effects model can be written

$$Y = X\beta+\epsilon, ~~\epsilon\sim N(0,\sigma^2 I_n).$$

The global score test statistic is straightforward to calculate, and can be written as

$$S_F = \frac{1}{\sigma^2}\text{tr}([X(X^TX)^{-1}X^T][YY^T]).$$

In practice, the nuisance parameter \(\sigma^2\) can be replaced with
\(\hat \sigma^2 = Y^TY/(n-1)\), which is the REML estimator when there
are no adjustment covariates. This is a scalar that is fixed under
permutations of \(K_F\). Therefore,
\[S_F \asymp Y^T X(X^TX)^{-1}X^TY = \text{tr}(HK_F),\]
where $H = YY^T$.

Now consider the random effects model

$$Y = Xb + \epsilon,$$

\noindent where \(b \sim N(\mathbf{0}, \sigma_b^2I_p)\), \(\epsilon \sim N(\mathbf{0}, \sigma^2I_n)\). Under the null hypothesis of no association between $X$ and $Y$, we have $\sigma_b^2=0$. Similar to @liu2007semiparametric, we first calculate the score and use the term that involves both $X$ and $Y$ as our score test statistic. Thus a testing equivalent form for the random effect score test statistic is

$$S_R \asymp \text{tr} (HK_R)/\sigma^2\asymp \text{tr} (HK_R).$$

\noindent The relationship between similarity-based tests and score tests of random-effect or kernel machine regression has been previously discussed in detail (@kwee2008powerful, @tzeng2009gene, @pan2011relationship).

The ridge regression model is a penalized form of the fixed effects model, for which the estimator of
\(\beta\) is defined as the minimizer of the penalized residual sum of squares:
\[\hat\beta_{\lambda} = {\arg \min}_{b} \left\{||Y-Xb||^2+\lambda||b||^2\right\}.\]

The above penalized function does not correspond to a likelihood conditional on
\((X,Y\)), but it can be formulated from the likelihood for the augmented data model

$$Y_\lambda = X_\lambda \beta + \epsilon$$

$$Y_{\lambda}=\begin{pmatrix}
Y\\
\mathbf{0}_{p\times 1}
\end{pmatrix},$$
$$X_\lambda = \begin{pmatrix}
X\\ \sqrt{\lambda} I_p
\end{pmatrix}$$

From this augmented likelihood, we can derive the score statistic

\[S_\lambda=\frac{1}{\sigma^2} Y_\lambda^T X_\lambda (X_\lambda^T X_\lambda)^{-1}X_\lambda^T Y_{\lambda}=\frac{1}{\sigma^2}\text{tr}(HK_{\lambda})\asymp \text{tr} (HK_\lambda).\]


#### (B)ridging the Fixed-effects and Random-effects Models

In this section we demonstrate that \(T\) is closely related to
well-known correlations in several linear models.
The sample correlation of the similarity measures of all subjects is
defined as

\begin{equation}
r(H,K)=\frac{tr(HK)}{\sqrt{tr(HH)tr(KK)}}
\end{equation}

Since \(r(H,K)\) is a correlation, we have \(-1 \leq r(H, K) \leq 1\) in
general, and for \(H\) and \(K\) p.s.d., this is further restricted to
\(0 \leq r(H,K) \leq 1\). When permutations are
used to assess the strength of association, using \(MT\) is equivalent
to testing the significance of \(r(H,K)\), since
the denominator of \(r(H,K)\) is fixed when
simultaneously permuting the rows and columns of either \(H\) or \(K\).
The sample correlation of similarities for the three choices of $K$ above can be conveniently related through the singular value decomposition (SVD) of $X$.

\noindent \textbf{Theorem.} Let $X$ be an $n \times p$ column centered matrix of covariates with $\text{rank}(X) = r$ and singular value decomposition $X = U_{n\times r}D_{r\times r}V_{p\times r}^T$, with squared singular values $\eta_i, i = 1, \cdots, r$.
Let $Y$ be an $n \times 1$ centered vector of scalar responses, and let \(H=YY^T\) and $Z = U^TY$.
\begin{enumerate}
	\item 
	Fixed Effects
	  \begin{equation}
    	r(H, K_F) = \frac{\sum_{i=1}^r z_i^2}{\sqrt{p} \sum_{i=1}^n y_i^2}.
    	(\#eq:corr-fixed)
    \end{equation}
	\item Random Effects
	  \begin{equation}
  		r(H, K_R) = \frac{\sum_{i=1}^r\eta_i z_i^2}{\sqrt{\sum_{i=1}^r\eta_i^2}\sum_{i=1}^n y_i^2}
  		(\#eq:corr-random)
    \end{equation}
  \item Ridge Regression
    \begin{equation}
		  r(H, K_{\lambda}) = \frac{\sum_{i=1}^{r} \frac{\eta_i}{\lambda + \eta_i}z_i^2}{\sqrt{\sum_{i=1}^{r} \left(\frac{\eta_i}{\eta_i+\lambda}\right)^2}     \sum_{i=1}^n  y_i^2}.
  		(\#eq:corr-ridge)
    \end{equation}
  \item Relation to Correlation of $X$ and $Y$
    	$$r(H, K_F) = \frac{1}{\sqrt{p}}R^2(X, Y)$$
   \item Asymptotic Equivalences
   		\[\lim_{\lambda \to 0} r(H, K_{\lambda}) = r(H, K_F)\]
   		\[\lim_{\lambda \to \infty} r(H, K_{\lambda}) = r(H, K_R).\]
\end{enumerate}

The preceding theorem also gives a straightforward derivation of the null distributions for the score tests.  Since the matrix $U$ from the SVD of $X$ is orthogonal, the distribution of $Z$ is
$$Z = U^TY \sim N_r(0, U^T \Sigma U),$$
where $\Sigma \equiv Cov(Y)$.  The asymptotic distributions of the score test statistics then follow from standard results on the distribution of quadratic forms of normal random variables.

\noindent \textbf{Corollary.} \textbf{Connections between the Mantel test statistics}

Let $V_i \sim \chi^2_1, i = 1, \cdots, r$ and $V \sim \chi^2_p$. 

\begin{enumerate}
    \item Fixed-effects model.
    	\[S_F \asymp \text{tr}(HK_F) = Z^T D(D^TD)^{-1}D^T Z ~\sim ~c V\]
    \item Random-effects model.
    	\[S_R \asymp \text{tr}(HK_R) = Z^TDD^TZ ~\sim~ \sum_{i = 1}^r \eta_i V_i ,\]
    \item Ridge regression.
      \begin{align*}
      S_{\lambda} &\asymp \text{tr}(HK_{\lambda})=\sum_{i=1}^{r}\frac{\eta_i}{\lambda+\eta_i}z_i^2 \propto \lambda \sum_{i=1}^{r}\frac{\eta_i}{\lambda+\eta_i}z_i^2 \\
      ~~&~\overset{\lambda \rightarrow \infty} \rightarrow \sum_{i=1}^r \eta_i z_i^2 \sim~ \sum_{i = 1}^r \eta_i \chi_1^2
      \end{align*}
\end{enumerate}

These results provide a description of the ridge regression score test as a natural intermediary between the fixed and random effects score tests.  A crucial difference of the ridge test is the presence of the tuning parameter $\lambda$. It is obvious that when $\lambda=0$, $S_\lambda$ reduces to $S_F$, and $r(H, K_\lambda)$ reduces to $r(H, K_0)$. On the other hand, when $\lambda \rightarrow \infty$, the test based on a ridge-penalty converges to that based on the random-effect model. Note that in permutation based methods, multiplying a constant does not change the permuted p-value. Therefore, $S_\lambda\propto \lambda\text{tr} (HK_\lambda)$, which converges to $\sum_{i=1}^r \eta_i z_i^2$, i.e., $S_R$.

\noindent {\bf Remark} To better understand the differences in the tests based on $T^*_F$ and $T^*_R$, a geometric comparison is helpful. Recognizing $K_F$ as the projection into $\mathcal{C}(X)$ and writing $T^*_F := \text{tr}(HK_F) = \|X(X^TX)^{-1}X^T Y\|^2$, we can interpret the test of $T^*_F$ as testing the norm of projected image of $Y$ into $\mathcal{C}(X)$.  Equivalently, $T^*_F$ tests the significance of the norm $\|X^TY\|_{\mathcal{W}},$ which is the Mahalanobis norm, since $\mathcal{W}$ is the Fisher information matrix of the fixed effects model. In contrast, the statistic $T^*_R := \text{tr}(HK_R)$ tests the unweighted $L_2$ norm of $X^TY \in \mathcal{C}(X)$.  Since the $j$th component of the $p \times 1$ vector $X^TY$ is the covariance of the $j$th feature of $X$ with $Y$, the test of $T_R^*$ can be understood as giving equal weight to each feature in $X$ in measuring the strength of the overall relationship of $X$ and $Y$, whereas the test of $T_F^*$ weights the contribution of each feature of $X$ according to the Mahalanobis norm.  In the case of independent features, these weights are inversely proportional to the observed variance of that feature.

The correlation formulas (\@ref(eq:corr-fixed)) -- (\@ref(eq:corr-ridge)) give some additional insight into the relationship of the three models in terms of $Z = U^TY$, the image of $Y$ after transformation by the left eigenvectors of $X$.  From \@ref(eq:fixed), we see that the fixed effects score test is equivalent to testing the Euclidean norm of $Z$.  Whereas from \@ref(eq:random), the random effects score test statistic is equivalent to testing the \textit{weighted} norm of $Z$, where the $j$th component (corresponding to the $j$th eigenvector) is weighted by $\eta_j$.  This has the effect of emphasizing the influence of directions in $\textbf{X}$ for which $X$ has large variance and reducing the influence of directions with small variance.  The ridge regression score test is a compromise between the fixed and random effects, with small $\lambda$ yielding a test close to the fixed effects (or identical at $\lambda = 0$), and large $\lambda$ yielding a test close to the random effects score test, and identical tests for $\lambda \to \infty$.  Geometrically, the ridge test weights the $z_j$ proportional to the $\eta_j$ as in the random effects test, but flattens each weight by a factor of $\frac{1}{\lambda + \eta_j}$.


#### Mantel Test for Multivariate Outcomes

##### Fixed Effects Model for Multivariate Response
Given the connection we established between fixed-effect and random-effect models, we are interested in extending the preceding results to the relationship between two sets of multivariate measurements. This extension is 
essential for many applications, such as heritability analysis of multi-dimensional traits. Suppose now that each subject $i$ is measured on a $q$-variate phenotype $Y_i$ so that $Y$ is an $n \times q$ matrix. Let $X$ be an $n \times p$ covariate matrix as before.The fixed-effect model assumes $Y\sim N(XB, \Sigma_e)$, where $\Sigma_e$ is a $q\times q$ positive definite matrix. The score statistic can also be expresses as a Mantel statistic where the similarity matrices for both $X$ and $Y$ are their corresponding projection matrices:
$$S_F \asymp \text{tr}(H_FK_F),$$
where $K_F=X(X^TX)^{-1}X^T$ and $H_F=Y(Y^TY)^{-1}Y^T$. 
An interesting observation is that  both similarities used in $S_F$ are based on the projection matrices. In the literature of multivariate linear models, several correlations have been proposed to quantify the overall correlation between $X$ and $Y$. For example, Hooper's trace correlation \cite{hooper1959simultaneous} is defined as 
$$r_T^2 = \frac{1}{q}\text{tr}\left((Y^TY)^{-1}Y^TX(X^TX)^{-1}X^TY)\right)$$
The relationship between Hooper's trace correlation and $r(H_F,K_F)$ follows immediately: 
$$r_T^2 = \sqrt{\frac{p}{q}}r(H_F, K_F).$$}

##### Random Effects Model for Multivariate Response
In the random-effect model for univariate outcomes, the $p$ coefficients are assumed to be i.i.d. random variables from a univariate normal distribution with a mean of zero. Extending this assumption to multivariate outcomes, we assume the $p$ vectors of coefficients, each is of length $q$ are i.i.d. random vectors from a multivariate normal distribution. In other words, the coefficients matrix follows a matrix normal distribution and the outcomes can be modeled as 
$$Y=XB+\epsilon,$$
where $B\sim N_{p\times q}(\mathbf{0}, \Sigma_b, I_p)$, $\epsilon \sim N_{n\times q}(\mathbf{0}, \Sigma_e, I_n)$ where $\Sigma_b$ and $\Sigma_e$ are  positive semidefinite matrices. In genetic studies, it is often interesting to evaluate the ``overall'' heritability of the multivariate outcomes. One heritability measure is based on the traces of the total variance in $Y$, which is summarized by $tr(\Sigma_b\otimes XX^T + \Sigma_e\otimes I_n)$, and the amount explained by $X$, which is $tr(\Sigma_b\otimes XX^T)$:
$$h^2=\frac{tr(\Sigma_b\otimes XX^T)}{tr(\Sigma_b\otimes XX^T + \sigma_e\otimes I_n)}=\frac{tr(K) tr(\Sigma_b)}{tr(K) tr(\Sigma_b) + tr(\Sigma_e)}$$
Using moment matching, \cite{ge2016multidimensional} provided the following MOM estimate:
$$\hat h^2_{MOM}=\frac{tr(K_R)tr(\hat \Sigma_b)}{tr(H_R)}=\frac{tr(K_R)}{tr(H_R)} \frac{tr(H_RK_R)-tr(H_R)tr(K_R)/n}{tr(K_R^2)-tr^2(K_R)/n},$$
which is approximately $$\frac{tr(K_R)}{tr(H_R)}\sqrt{\frac{tr(H_R^2)}{tr(K_R^2)}}r(H_R,K_R).$$
When both $X$ and $Y$ are column-standardized and assumed that their ranks are the same as their number of columns, we have $tr(H)=np$ and $tr(K_R)=np$. It can be shown that 
$$\hat h^2_{MOM}\in \left[\frac{1}{\sqrt{q}}r(H_R,K_R),\sqrt{p}r(H_R,K_R)\right]$$

##### Ridge-Penalized Multivariate Linear Models
In Section \ref{univariate_ridge} we described how the ridge regression bridges linear fixed- and random-effect models. Note the in the ridge regression we replace $X^TX$, which is singular in high-dimensional settings, with $X^TX+\lambda I$. The consequence of adding the same positive number to each diagonal element is to de-emphasize the importance of the variance covariance structure of $X$. For example, for large enough $\lambda$, $X^TX+\lambda I$ is dominated by $\lambda I$. In many problems, such as those imaging genetics, the outcome variables can also be high dimensional. For example, in the example we will present in Section ***, the number of outcomes (brain connectivity)is large. This motivates us to consider a more general ridge penalization, which uses tuning parameters for both $X$ and $Y$. It has been found that, when taking the dependence structure in $Y$ into consideration, ridge regressions can be very useful tool for simultaneous prediction of multiple outcome variables (\cite{brown1980adaptive,breiman1997predicting}). Here we focus on the setting where both the independent and the outcome variables are high dimensional. Intuitively, we would like to use two tuning parameters, $\lambda_x$ and $\lambda_y$ when defining the similarity measures used in Mantel test:
\begin{equation}
H_{\lambda_y}=Y(Y^TY+\lambda_y I_q)^{-1}Y^T \mbox{ , } K_{\lambda_x}=X(X^TX+\lambda_x I_p)^{-1}X^T
\label{eqn: HK_lambda}
\end{equation}

Similar to the univariate case, the ridge-penalize regression multivariate regression also connects fixed- and random-effect models through correlations. 
\noindent \textbf{Corollary.} \textbf{Connections between the correlations}.
$$\lim _{\lambda_x\rightarrow 0, \lambda_y\rightarrow 0} r(H_{\lambda_y},K_{\lambda_x})=r(H_F, K_F)$$
$$\lim _{\lambda_x\rightarrow \infty, \lambda_y\rightarrow \infty} r(H_{\lambda_y},K_{\lambda_x})=r(H_R, K_R)$$

To understand the underlying statistical implication of the measures, we consider data augmentation. Data augmentation has been widely used to ease the computation in several problems, such as the expectation and maximization algorithms, and Gibbs or MCMC sampling \cite{duncan1972linear, dempster1977maximum, dempster1981estimation, tanner1987calculation, van2001art, gelman2014bayesian}. In particular, (\cite{hodges1998some}) described how data augmentation can be used as a ``computing device" so that hierarchical models can be written in the format of linear models. It can be verified that the data augmentation in corresponds to the similarity measures in Equation \@ref(eqn: HK_lambda} is   
\begin{equation}
\tilde{Y}=\begin{pmatrix}Y\\ \mathbf{0}\\ \sqrt{\lambda_y} I_q\end{pmatrix}, \tilde{X}=\begin{pmatrix}X\\ \sqrt{\lambda_x} I_p \\ \mathbf{0}\end{pmatrix}
\end{equation}
Let $\tilde{H}=\tilde{Y} (\tilde{Y}^T\tilde{Y})^{-1}\tilde{Y}^T$, $\tilde{K}=\tilde{X} (\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T$. 
The least squares estimate (based on the augmented data) of $B$ is
$$\hat B=(\tilde X^T\tilde X)^{-1}\tilde X^T \tilde Y=(X^TX+\lambda_x I)^{-1}X^TY,$$
which only depends on the tuning parameter for $X$. To see the effect of the tuning parameter $\lambda_Y$, we examine the log-likelihood based on the augmented data when the assumptions of the fixed-effect model is used:
\begin{equation}
l=-\frac{(n+p+q)q}{2}log(2\pi) - \frac{n+p+q}{2}log|\Sigma_e| - \frac{1}{2}tr[(Y-XB)\Sigma_e^{-1}(Y-XB)^T] - \frac{\lambda_x}{2}tr[B\Sigma_e^{-1}B^T]-\frac{\lambda_y}{2} tr[\Sigma_e^{-1}]
\label{eqn: neglik_augment}
\end{equation}
At first glance, the two tuning parameters seem to have different effects. While $\lambda_x$ shrinks $B$ toward zero, $\lambda_y$ encourages smaller sums of the reciprocals of the eigenvalues of $\Sigma_e$. As discussed in (***?), shrinking $B$ by the ridge penalization induced by $\lambda_x$ is equivalent replace $X(X^T+\lambda_x I_p)^{-1}X^T$ with $XX^T$ when $\lambda_x$ is large. The penalization through $\lambda_y$ encourages a small sum of the reciprocals of the eigenvalues of $\Sigma_e$; equivalently, it is encouraged that $\Sigma_e$ has large and equally-sized eigenvalues. When $\lambda_y$ is large, we expect that $\Sigma_e$ is proportional to an identity matrix, which means we can replace $Y(Y^TY+\lambda_y)^{-1}Y^T$ with $YY^T$ (up to some scalar). Thus, large values of tuning parameters favors the use of Euclidean distance, which ignores the dependence structure with $X$ or $Y$.

Another observation is that the negative likelihood function in Equation \@ref(eq: neglik_augment} is mathematically identical to the posterior distribution of $B$ and $\Sigma_e$ where the joint prior is a normal-inverse-Wishart distribution:
\begin{eqnarray}
Y|B,\Sigma_e &\sim& N_{n\times q}(XB, \Sigma_e, I_n)\\
p(B,\Sigma_e) &\propto&  |\Sigma_e|^{\frac{p+q}{2}}exp\{-\frac{\lambda_x}{2}tr[B\Sigma_e^{-1}B^T]-\frac{\lambda_y}{2} tr[\Sigma_e^{-1}]\}
\end{eqnarray}
We have showed that it is reasonable to apply a ridge parameter to each of the sets of variables, namely $X$ and $Y$. Similar to the univariate case, we can define a penalized Mantel test statistic:   $$S_{\lambda_x,\lambda_y}=tr(H_{\lambda_y} K_{\lambda_x})$$
The overall association between $X$ and $Y$ can be evaluated using the adaptive strategy we proposed in ***. 

#### Adaptive Mantel Test

Effectively using kernel methods requires an appropriate selection of the kernel function and tuning parameters for the particular setting.  Selection methods have been extensively considered in the context of prediction problems, with 
cross-validation as the \textit{de facto} standard.  Cross-validation is a straight-forward and practical selection method for prediction, but may be difficult to implement for hypothesis testing, since the type I error rate needs to be controlled.  Furthermore, the tuning parameter selected by minimizing the CV MSE may not necessarily yield the highest powered test. This section introduces the adaptive Mantel test (AMT), which extends the classical MT to simultaneously test across a set of tuning parameters and kernels without the need to directly apply adjustments for multiple comparisons.

The ``adaptive'' procedure used here is similar to the adaptive sum of powered score test algorithm described in @xu2017adaptive. The procedure receives as input a list of pairs of metrics/kernels $\{(\delta^{\textbf{X}}_{m}, \delta^{\textbf{Y}}_{m}) | m = 1 , \cdots M\}$ from which the matrices $K_m = \delta^{\textbf{X}}_{m}(X)$ and $H_m = \delta^{\textbf{Y}}_{m}(Y)$ are computed for each metric pair, $m = 1, \cdots, M$.  These metrics may be from a single family with varying tuning parameters, such as ridge kernels with different penalization terms, or may include kernels from different families.

For each $m = 1, \cdots, M$, $P_m$ is calculated as the $P$-value of the Mantel test with metrics $\delta_m^{\textbf{X}}$ and $\delta_m^{\textbf{Y}}$ for $X$ and $Y$ respectively.  The AMT test statistic is defined as the minimum of these values,

$$P^{(0)} := \min_{m = 1, \cdots, M} P_m.$$

A permutation procedure can be used to calculate the reference distribution for $P^{(0)}$.  For each $m$, and  $b = 1, \cdots, B$, $H_m^{(b)}$ is generated by permuting rows and columns of $H$ simultaneously, and the corresponding test statistic $P^{(b)}$ is calculated.  The AMT $P$-value is then calculated as

$$P_{AMT} = \frac{1}{B + 1} \sum_{b = 0}^B I\left(P^{(0)} \leq P^{(b)}\right).$$

```{r, echo = FALSE, fig.width = 4, fig.height = 6, fig.align='center'}
img <- png::readPNG("img/adamant_algorithm.png")
grid::grid.raster(img)
```


<!-- General pseudocode for the adaptive Mantel test is given in Algorithm 1.   -->

<!-- \begin{algorithm} -->
<!-- \caption{Adaptive Mantel algorithm}\label{alg:adamant} -->
<!-- \begin{algorithmic}[1] -->
<!-- \FOR {$m = 1, \dots, M$} -->
<!-- \STATE $K_{m} \gets \delta^{\textbf{X}}_{m}(X)$ -->
<!-- \STATE $H_m \gets \delta^{\textbf{Y}}_{m}(Y)$ -->
<!-- \STATE Calculate $Z_{m}^{(0)} \gets Z_{m} := \text{tr}(K_mH_m)$ -->
<!-- \ENDFOR -->
<!-- \STATE Generate $B$ permutations of $H_m$, labeled $H_m^{(b)}~~\forall~m = 1, \dots, M; b = 1, \dots, B$. -->
<!-- \STATE $Z_{m}^{(b)} \gets \text{tr}(K_{m}H_m^{(b)})~~\forall~m = 1, \dots, M; b = 1, \dots, B$ -->
<!-- \STATE $P_{m}^{(b)} \gets \frac{1}{B + 1}\sum_{b = 0}^B I\left(Z_{m}^{(b)} \leq Z_{m}^{(b')}\right)~~\forall~m = 1, \dots, M; b = 1, \dots, B$  -->
<!-- \STATE $P^{(b)} \gets \min_{m = 1, \cdots, M} P_{m}^{(b)}~~\forall~b = 1, \dots, B$ -->
<!-- \STATE $P_{AMT} \gets \frac{1}{B + 1} \sum_{b = 0}^B I\left(P^{(0)} \leq P^{(b)}\right)$ -->
<!-- \end{algorithmic} -->
<!-- \end{algorithm} -->

### Aim 1.2: Extensions of the Adaptive Mantel Test

Given the recent success of kernel methods for regression and prediction, there are likely many application settings for which a Mantel test 
with a non-linear similarity or distance metric will be more appropriate and effective than the linear metrics previously discussed.  Kernel 
methods have been thoroughly developed for prediction problems, but, as with ridge regression, the use of kernels in inference settings have received 
relatively little attention.  Through the use of the adaptive Mantel test, the difficulties of tuning parameter selection for inference can be partially mitigated, 
making it practical to test for the association of two sets of features for which the similarity of subjects is measured via a kernel similarity function rather 
than the typical Euclidean metric.

We here mention two kernels that are of interest for imaging genetics, namely the Gaussian radial basis function (RBF) and the identity by descent (IBD) kernels.  The RBF kernel is defined as

$$\mathcal{K}(x, x') = \exp\left(-\lambda\|x - x'\|^2\right),$$

where $\|x - x\|^2$ is the squared Euclidean distance and $\lambda$ is a bandwidth tuning parameter.

When comparing the genetic relationship of two (unrelated) individuals, a given DNA segment is said to be _identical by descent_ if the individuals have inherited the segment from a common ancestor.  The IBD kernel measures the similarity of two individuals as the average identity by descent across a set of SNPs.

When a observational modality has a smooth manifold structure, it is also possible to define an inner product on the manifold to use as a similarity measure.  In general. this approach is distinct from the kernel approach, in that the intrinsic manifold metric will induce curvature on the manifold, and will not be representable as a transformation of a Euclidean measure, whereas a kernel-based inner product will a representation of a weighted Euclidean inner product.  The manifold approach may offer some advantage by utilizing the natural structure of the observed data, thus achieving some dimensionality reduction relative to using the Euclidean metric.

Yet another approach, particularly relevant for analysis of connectivity data, is the use of graph-theoretic similarity measures, which measure the similarity of graph topology and structure according to different properties, such as average degree, cut-norm, girth, eccentricity, average clustering coefficient, small-worldness, etc.  Each of these metrics emphasize different aspects of graph structure, thus through the combination of these measures, one can develop similarity measures on graphs that are more interpretable than using the Frobenius norm or similar on the adjacency matrix.

While the above metrics can easily be used as part of a Mantel test, the theoretical properties of using these metrics has not yet been developed, and so it would be difficult to apply and interpret the results of such a test in a rigorous fashion.  Building on the results of our work on the adaptive Mantel test, we propose to assess the properties of the Mantel test when different metrics are chosen, and determine which choice of metrics is most appropriate for answering particular scientific questions in imaging genetics studies.

### References

- @kwee2008powerful
- @schaid2010genomic
- @schaid2010genomic2
- @mantel1967
- @robert1976unifying
- @xu2017adaptive

\newpage

## Aim 2: Manifold Regression Models for Imaging Genetics

### Aim 2.1: Manifold Regression for Connectivity Differences

Although the foundational ideas for manifold regression models are very old, reaching back to Fisher's development of regression models for circular data, there has been a renewed interest in models for manifold-valued data in recent years. Initially spurred by impressive progress in predictive settings with machine learning methods, these ideas have been developed into tools for the analysis of covariance in time series, and neuroimaging applications.  The work by Zhu et. al (@zhu2009intrinsic) presents a semiparametric regression model for manifold-valued responses, for application to diffusion tensor imaging data.  To map the structural connectivity of the brain, DTI measures a $3\times 3$ positive-definite matrix at each voxel, which captures the degree and direction that water molecules tend to diffuse along white matter tracts in the brain.  If the goal is to estimate differences in connectivity structure adjusting for covariates, classical multivariate regression can be applied, but this introduces technical and theoretical difficulties, since the resulting estimates must be modified in some way to ensure that the intrinsic constraints of the observation space are respected when making predictions, i.e., one has to somehow guarantee that predicted values are in the space of positive-definite matrices.  This is a common difficulty in modeling and estimating characteristics of positive-definite matrices.  A theoretically natural alternative then is to model the data on the PD manifold, denoted instead of in Euclidean space.  

As an example, the following model is proposed by Zhu et al. as one possibility.  Suppose a $q \times q$ PD matrix $S_i \in \text{Sym}^+(q)$ and a $k \times 1$ vector of covariates are observed for each subject $i = 1, \cdots, n$.  Let $\beta \in \mathbb{R}^p$ be a $p \times 1$ vector of regression coefficients, and $\Sigma(\cdot, \cdot): \mathbb{R^k}\times\mathbb{R^p} \to \text{Sym}^+(q)$. We are interested in modeling the "conditional mean" of $S_i$ given $x_i$, denoted $\Sigma_i(\beta) = \Sigma(x_i, \beta) \in \text{Sym}^+(q)$.  To state the "geodesic" model for $\Sigma$, let $D \in \text{Sym}^+(q)$ be the intercept matrix $D = \Sigma(0, \beta)$, with $D = BB^T$ for some $B \in GL(q)$, let $Y_D(x_i, \beta) = Y_{D, i} \in \text{Sym}(q)$ be a "directional" matrix, and let $C_i(\beta)$ be a Cholesky square root $\Sigma(x_i, \beta) = C_i(\beta)C_i(\beta)^T$.  The geodesic model then assumes that

$$\Sigma(x_i, \beta) = B\exp(B^{-1}Y_{D, i}(\beta)B^{-T})B^T = C_i(\beta)C_i(\beta)^T.$$

Following a result in @schwartzman2006random, $\Sigma(x_i, \beta)$ is related to the geodesic through $D$ in the direction of $Y_{D, i}$.  The residual of $S_i$ with respect to $\Sigma_i(\beta)$ from this model is defined as

$$\mathcal{E}_i(\beta) = \log(C_i(\beta)^{-1}S_iC_i(\beta)^{-T}).$$

The intrinsic regression model is then specified by $\mathbb{E}[\mathcal{E}_i(\beta) | x_i] = 0$, where the intrinsic least squares estimate of $\beta$ is defined as 

$$\hat \beta = \arg\min_{\beta}\sum_{i = 1}^n \text{tr}\left(\mathcal{E}_i(\beta)^2\right).$$



### Aim 2.2: Mixed Effects Manifold Regression for Heritability of Brain Connectivity

The classical mixed effects model has become a popular and powerful tool for analyzing longitudinal measurements of real-valued response data.  In the case of $n$ subjects measured at $T$ regularly spaced times, with response $y_{it}$ for subject $i$ at time $t$, fixed-effects covariates $X_i$, and random effects covariates $Z_i$ which link the subject specific random effects with the response, the linear mixed effects model can be written as
$$y_i = X_i\beta + Z_i b_i + \varepsilon_{i},$$
where $b_i \sim N(0, D)$ is a vector of subject specific random effects, and $\varepsilon_i \sim N(0, R)$ is a vector of error terms that are assumed to be independent and identically distributed across subjects, and with $b_i$ and $\varepsilon_i$ assumed independent (@laird1982random).  

Kim et al. (2017) have proposed an extension of the linear mixed effects model for manifold-valued responses.  Let $Y_{[ij]}, B, B_i \in \mathcal{M}, V \in T_B\mathcal{M}^p, U_i \in T_{h[ij]}\mathcal{M}^q, x_{[ij]}\in \mathbb{R}^p, z_{[ij]} \in \mathbb{R}^q$, and let $\Gamma_{B \to B_i}V$ be the parallel transport of $V$ from $B$ to $B_i$.  A simplified form of the model can be stated as

$$Y_{[ ij ]} = \text{Exp}(\text{Exp}(B_i, \Gamma_{B\to B_i} (V)x_{[ij]}, \varepsilon_{[ij]}))$$
$$B_i = \text{Exp}(B, U_i).$$

Similar to the classical setting, the random effects $U_i$ are assumed to follow a normal distribution,

$$\Gamma_{B \to I} U_i ~\sim~ \mathcal{N}_{SYM}(0, \sigma_U^2),$$

where $\mathcal{N}_{SYM}$ is the normal distribution over symmetric positive definite matrices.

Analagous to the definition of the random effects model for heritability analysis, we propose a random effects model for manifold-valued responses measured from a set of unrelated subjects with SNP matrix $X$ and genetic relationship matrix $G = XX^T/p$.  For this model, we instead consider the random effects $U_i$ to have a common aggregate genetic effect $\sigma^2_U$, such that the contribution of this effect to each subject's response is proportional to the genetic relationship matrix, giving random effect distribution $\Gamma_{B \to I} U ~\sim~ \mathcal{N}_{SYM}(0, \sigma^2_U G)$.

### References

- @kim2017riemannian
- @yang2011gcta
- @laird1982random


\newpage

## Aim 3: Multi-subject Models for Dynamic Connectivity

### Aim 3.1: HOSVD with switching factor-VAR

As a simple start to multi-subject modeling, the idea is to adapt the SF-VAR model (Samdin 2017)
to estimate common states across multiple subjects by replacing the K-means clustering step 
with the closely related HOSVD method as described in Huang 2008.  As far as I know (and according to Hernando), 
joint estimation of dynamic connectivity states across subjects is a still open problem, 
with most current methods being ad hoc.  HOSVD clustering should give a more principled,
but still intuitive, approach.

The first figure from Samdin 2017 gives the steps of the dynamic connectivity modeling for a single subject.  The second figure shows the proposed changes in magenta, which are primarily in 
Step 3, where the K-means clustering is replaced by HOSVD clustering.  


#### Step 1: Multivariate multi-subject Neural Signals $Y_{it}$, the activation of $Q$ EEG channels at time $t$ for subject $i$.

#### Step 2: Feature Extraction: TV-VAR coefficients

$$Y_{it} = \sum_{\ell = 1}^L \Phi_{i \ell t} Y_{i, t - \ell} + v_{it}$$

where $\Phi_{i \ell t}$ is the $Q \times Q$ matrix of VAR coefficients for lag $\ell$ at time $t$ and observational noise $v_{it} \sim N(0, \Sigma_{iv})$.

To write the state-space form, define $a_{it} = vec([\Phi_{it1}, \cdots, \Phi_{itL}]')$

$$a_{it} = a_{i,t - 1} + w_{it}$$
where $a_{it}$ is the hidden state, assumed to follow a first-order Gauss-Markov process, $w_{it} \sim N_{LQ^2}(0, \Sigma_{wi})$.

$$Y_{it} = C_{it}a_{it} + v_{it},$$

with $C_{it} = I_Q \otimes X_{it}^{\prime}$, where $X_{it} = [Y^{\prime}_{i,t - 1}, \cdots, Y^{\prime}_{i,t - L}]$ is a $QL \times 1$ vector of past observations.

Estimate $\hat a_{it} = \mathbb{E}(a_{it} | Y_{i})$ for each $i = 1, \cdots, N$ using the KF to adaptively extract the sequence of estimated TV-VAR coefficients.  From the estimated TV-VAR coefficient matrices we then  calculate the estimated PDC from region $q$ to region $q^{\prime}$ as

$$\hat \pi_{qq^{\prime}} (f) = \frac{|\Phi_{qq^{\prime}}(f)|}{\sqrt{\sum_{j = 1}^{Q}|\Phi_{jq^{\prime}}(f)|^2}}.$$


#### Step 3a: HOSVD Clustering into Distinct Regimes

Let $\hat \Pi$ be the $N \times Q \times Q$ tensor of estimated PDC matrices for each subject.  
Apply $K$-means tensor clustering by mimizing

$$\min_{\{C_k\}} \sum_{i = 1}^N \min_{1 \leq k\leq K} \|M_i - C_k\|^2 = \sum_{k = 1}^K \sum_{c \in C_k} \|M^{(k)}_{i} - C_k\|^2,$$

where $C_k$ is the centroid of cluster $k$ and $M^{(i)}$ is the loading matrix for subject $i$.  
By Huang 2008, this is equivalent to solving

$$\min_{U, V, M} J = \|\hat\Pi - U\otimes V \otimes M\|^2 = \sum_{i = 1}^n \|\hat\Pi_i - U\otimes V \otimes M_i\|^2.$$ 

This results in $K$ clusters which can then be identified with $K$ latent brain states that are
common across subjects.  This can also be written as

$$\hat \Pi_i = U \otimes V \otimes M_i$$

where $U$ and $V$ are fixed for all subjects, and $M_i$ is the subject-specific "core" matrix.

If desired, it is also possible to do dimension reduction in the channel-channel space simultaneously with the $K$-means clustering step.  This requires the choice of 
reduction rank $R$, which could possibly be chosen usinig AIC/BIC or similar.

#### Step 3b: Reconstruction of signals from low-rank approximation from (3a).

#### Step 4: Segment signals for model initialization with a state sequence $\hat S_t^{KM}$.

#### Step 5: Refine estimates within each regime using a Markov-SVAR model:

$$Y_{it} = \sum_{\ell = 1}^L \Phi_{i\ell}^{[S_t]}Y_{i,t - \ell} + v_{it}.$$

#### Step 6: Assume state transitions for each subject $i$ follow a hidden Markov chain 
with transition matrix $Z_i = [z_{ijk}], 1 \leq j, k\leq K$, where

$$z_{ijk} = P(S_{it} = j | S_{i,t - 1} = k)$$

denotes the probability of transition from state $k$ at time $t - 1$ to state $j$ at time $t$.


\newpage

### Aim 3.2: Inference and Estimation of Covariate Effects on Dynamic Connectivity

#### Aim 3.2.1: Regression Analysis of Multi-subject Dynamic Connectivity

Given robust estimates of common states across subjects, there are a 
number of scientifically important questions we can consider, such as 
the significance of genetic factors, and also the relevance of other particular 
covariates.

##### Task-based Dynamic Connectivity Analysis

For a task-based experiment, there are many things we can test about the 
dynamic connectivity states.  For instance, is the task-domain significantly 
related to the identified states, are other covariates significant, such as gender, reaction time,
performance during the task, etc.  We could conduct the regressions with the response as either the state indicators at each time point, or the state transition probabilities.

For each subject $i$, pair of states $k, k'$, and time $t$ such that $S_{i, t - 1} = k$, define $\zeta_{it} = 1$ if $S_{it} = k'$ and $\zeta_{it} = 0$ if $S_{it} \neq k'$.  Suppose each subject has $M$ 
clinical covariate data $U_{i}$, forming $N \times M$ data matrix $U$.  Let $\zeta$ denote 
the $NT \times 1$ vector concatenating each $\zeta_{it}$.

$$\text{logit}(\zeta) = \beta U.$$


References:

  - @samdin2017unified
  - @ting2017multi
  - @taghia2017bayesian
  - @BayesianVAR
  - @hutchison2013dynamic
  - @havlicek2010dynamic

\newpage

## Aim 4: Broad Scientific Impact: Data Analysis and Software Development

### Aim 4.1: Identifying Significantly Heritable Features of Static EEG Coherence

### Aim 4.2: Applications of Manifold Random Effects Heritability Model

### Aim 4.3: `R` Package for Adaptive Mantel Test

#### AdaMant Package

- Current version: https://github.com/dspluta/adamant

##### Installation

1. Install using the `devtools` package:

```{r, eval = FALSE}
install.packages(c("devtools", "tidyverse"))
devtools::install_github("github.com/dspluta/adamant")
```

2. After installation, load the package as usual:

```{r, cache = TRUE, warning = FALSE, message = FALSE, include = FALSE}
library(adamant)
```

##### **Example 1**: $n = 50, p = 10$

```{r, cache = TRUE, warning = FALSE, message = FALSE}
# Generate Example Data: n = 50, p = 10
set.seed(1234)
n <- 50
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- X %*% rep(c(0, 0.6), p / 2) + rnorm(n, 0, 2)

# Apply Adaptive Mantel Test
adamant(X, Y, lambdas_X = c(0, 1, 10, Inf),   
        n_perms = 2000, P_val_only = TRUE)
```

##### **Example 2:** $n = 500, p = 100$
```{r, cache = TRUE, warning = FALSE, message = FALSE}
# Generate Example Data: n = 500, p = 100
set.seed(1234)
n <- 500
p <- 100
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- X %*% rep(c(0, 0.06), p / 2) + rnorm(n, 0, 2)

adamant(X, Y, lambdas_X = c(0, 1, 10, Inf),   
        n_perms = 2000, P_val_only = TRUE)
```

##### **Example 3:** $n = 500, p = 1000$

```{r, cache = TRUE}
# Generate Example Data: n = 500, p = 1000
set.seed(1234)
n <- 500
p <- 1000
X <- matrix(rnorm(n * p), nrow = n, ncol = p)
Y <- X %*% rep(c(0, 0.03), p / 2) + rnorm(n, 0, 2)

adamant(X, Y, lambdas_X = c(0.1, 1, 10, Inf),   
        n_perms = 2000, P_val_only = TRUE)
```

### Aim 4.4: `R` Package for Manifold Heritability Models

### Aim 4.5: `R` Package for Multi-subject Dynamic Connectivity Modeling

\newpage

## Technical Challenges

- Identification of common brain states across subjects
- Determining scientifically relevant targets of inference
- Computational issues in model fitting
- Sensible inclusion of covariates
- Sensible choice of priors for Bayesian modeling
- Data pre-processing
- Model assessment and validation

\newpage

# Coordination Plan

## Advancement Exam

- __Date:__ March 28th, 2018
- __Time:__ 3:00 pm PT
- __Committee__:
  + Zhaoxia Yu (Chair)
  + Hernando Ombao
  + Hal Stern
  + Dan Gillen
  + Michele Guindani
  + Chuansheng Chen (UCI, Dept of Social Ecology)

## Timeline

- _Aim 1:_ Substantial work toward this aim has been completed as of __January 30th, 2018__, culminating in the manuscript _Adaptive Mantel Test for Penalized Inference, with Applications to Imaging Genetics_, which introduces the adaptive Mantel test and describes the basic methodology for applying the test.  This manuscript will be submitted for publication by __February 28th, 2018__.  A thorough analysis of the BNU data with the adaptive Mantel test will be prepared for publication, targetting a submission in __June 2018__.

- _Aim 2:_ Code for the single subject sf-VAR model has been made available by our collaborator Chee-Ming Ting.  This code will be adapted for multi-subject modeling, and applied to simulated and real-world data, with results available by __February 28th, 2018__. The method and results will be written up and submitted for publication by __July 2018__.

- _Aim 3:_ From the results of Aim 2, tests for significant genetic factors relating to dynamic connectivity features will be conducted, with results presented as part of the Advancement Exam, to be held on __March 28th, 2018__.  The method and results will be written up and submitted for publication by __September 2018__.

- _Aim 4:_ Model proposal and supporting background literature will be presented as part of the Advancement Exam, __March 28th, 2018__.  A simulation study will conducted in Summer 2018.  Further model development and application to real data will occur throughout 2018, targeting a manuscript submission in __January 2019__.


\newpage

## Research Personnel

- Dustin Pluta
    + https://dspluta.github.io/
- Zhaoxia Yu, Research Advisor
- Hernando Ombao, Co-advisor
- Gui Xue, BNU Principal Investigator
- Chuansheng Chen, Collaborator and BNU-UCI liason
- Chee-Ming Ting, Collaborator
- Tong Shen, UCI PhD student

\newpage

# Appendix

## Appendix: Connectivity Formulas

To define different connectivity measures, we consider $q$ time series each of length $T$, with each series centered.  Let $Y_{j}$ denote the $j$th time series, and $Y_{j}(t)$ value of the $j$th time series at time $t$.

### Pearson's Correlation

The (marginal) Pearson correlation between series $j$ and $k$ is

$$\rho = \frac{\text{Cov}(Y_j, Y_k)}{\sqrt{\text{Var}(Y_j)}\sqrt{\text{Var}(Y_k)}}.$$

### Coherence

The coherence between two EEG channels at a particular frequency $\omega$ is a measure of the oscillatory concordance of the the two signals at $\omega$.  The pairwise coherence for $q$ EEG channels is a $q \times q$ symmetric matrix, from which we extract the upper triangle and vectorize to form the $n \times {{q}\choose{2}}$ matrix $X$.

### Partial Directed Coherence

### Mutual Information

### Technical Considerations for Connectivity Calculations

## Appendix: Aim 1

### Computational Methods

If the feature space is very high-dimensional or if $n$ is large, a straightforward implementation of Algorithm 1 may be computationally impractical.  However, when only ridge kernels with varying values of $\lambda$ are included in AMT, there are two approaches that can be used to greatly reduce the computational cost.

The first approach utilizes the SVD $X = UDV^T$.  The computational complexity needed for finding the SVD for $X$ is $O(np^2)$. Once the SVD is computed, we can compute $Z=U^TY$ and $S_\lambda=\sum_{i=1}^r \frac{\eta_i}{\lambda+\eta_i}z_i^2$, which has a total complexity of $O(nr)$. Note that when $p\gg n$, the rank $r$ is often the same as $n$; as a result, the cost needed for calculating $S_\lambda$ is $O(nr)=O(n^2)$. Calculating the test statistics for $B$ permutations requires $O(Bn^2)$, for a total computational complexity of $O(np^2+ Bn^2)$. 

Alternatively, when $p$ is very large relative to $n$, we can use the identity $X(X^TX + \lambda I)^{-1} = (XX^T + \lambda I)^{-1}X,$ so that the matrix inverse is applied to an $n\times n$, rather than $p\times p$, matrix.  From this identity, $K_\lambda$ can be rewritten as 

$$K_\lambda = X(X^TX + \lambda I_p)^{-1}X^T$$
$$= (XX^T + \lambda I_n)^{-1}XX^T$$

Note that calculating $K_\lambda$ involves multiplying the $n\times p$ matrix $X$ and the $p\times n$ matrix $X^T$, multiplying two $n\times n$ matrices, and inverting an $n\times n$ matrix. When $p\gg n$, the computation cost is dominated by calculating $XX^T$, which has a complexity of $O(n^2p)$. The Mantel test statistic can be calculated as
$$S_\lambda = \text{tr}(YY^T K_\lambda)=Y^T K_\lambda Y,$$
which has a complexity of $O(n^2)$. With $B$ permutations, the total computational complexity is $O(n^2p + Bn^2)$, which is less than the required computational complexity using SVD. Thus, switching from the feature space to the subject space (i.e., from a $p\times p$ similarity matrix of the features to an $n\times n$ similarity matrix of the subjects), has a computational advantage.  Additionally, in some situations the SVD computation may be unstable, thus the matrix identity method may be recommended as the more robust approach. 

### Variance Explained and the Ridge Penalty

By allowing for simultaneous testing over a set of tuning parameter values, AMT lessens the challenge of parameter selection, but does not completely resolve it.  Although it does not require an overly conservative adjustment for multiple testing, the power of AMT does decrease as the number of metrics considered increases.  Conversely, the test results are highly sensitive to the choice of parameters to test.  Consequently, one must still take some care in the selection of the included parameters, balancing the desire to use a wide range of parameter values, with the gains of using a small set of parameters.  When only ridge kernels are included in AMT, previous results on the role of the ridge penalty term in predictive modeling can help with the identification of a reasonable set of values to test. Specifically, it has been shown that when the ridge penalty $\lambda$ is chosen to be the noise to signal ratio, the resulting shrinkage estimator $\hat\beta_\lambda$ is identical to the best linear unbiased predictor (BLUP) $\hat b$ for the random effects model, and moreover, for a new observation with unknown response value the predictions using the ridge and random effects models are the same (@de2013prediction).  Thus, when using ridge regression for prediction, it is recommended that the penalty should reflect the relevant level of noise versus signal, i.e., $\lambda=\sigma_e^2/\sigma^2$. 

To apply this result to practical settings, if one can determine \textit{a priori} a likely range for the noise to signal ratio or a related quantity, this will determine a reasonable range of penalty terms. For instance, in assessing the genetic influence on observed phenotypes, the noise to signal ratio is related to what is known as the \textit{heritability} of the phenotype, which can be understood as the proportion of variance in the observed trait explained by the genetic data.  Formally, for (standardized) genetic data matrix $X$ consisting of alleles for $p$ single nucleotide polymorphisms (SNPs), and observed phenotype vector $Y$, the random effects model given in Eq. \@ref(eq:random} is commonly used to estimate the genetic heritability of the trait (@yang2011gcta, liu2007semiparametric). From this model, the \textit{narrow-sense heritability} $h^2$ of the phenotype is defined as 

$$h^2 := \frac{p\sigma^2}{p\sigma^2 + \sigma^2_{\varepsilon}}.$$

Plugging in $Var(Y) = \sigma^2 + \sigma^2_{\varepsilon}XX^T$ gives

$$h^2 = \frac{\sigma^2tr(XX^T/n)}{\sigma^2tr(XX^T/n) + \sigma^2_{\varepsilon} tr(I_n/n)} =\frac{tr(XX^T/n)}{tr(XX^T/n) + \sigma^2_{\varepsilon}/\sigma^2}.$$

We see then that if $h^2$ is known, the optimal penalty $\tilde \lambda$ (for prediction) can be found by solving for the noise to signal ratio.  In practice, the scientific interpretation and range of plausible values of $h^2$ will depend on the specific modalities of $\textbf{X}$ and $\textbf{Y}$. For instance, in the genetics literature, $h^2> 0.5$ would generally indicate high heritability, while a heritibility of $h^2 < 0.1$ is probably not scientifically interesting.  As a point of reference, most estimated heritability in the UK Biobank data is between 0.1 to 0.4 (@ge2017phenome).

### Asymptotic Results for the Penalized Mantel Test

Assume true model is the random effects 

$$Y = g + \varepsilon,$$
  
  where $g \sim \mathcal{N}(0, \sigma^2XX^T), \varepsilon \sim \mathcal{N}(0, \sigma^2_{\varepsilon})$.  Assume the SVD of $X$ is $X = U D V^T$, with singular values $d_j$ and eigenvalues $\delta_j = d_j^2, j = 1, \cdots, p$.  Further assume that $X$ and $Y$ have been centered and scaled, so that $\text{tr}(XX^T) = np$, and $\text{tr}(X(X^TX)^{-1}) = p$.
  
#### Random Effects Test
  
Let $K_R = XX^T$.

\begin{align}
T_R &= \text{tr}(XX^T) = \sum_{j = 1}^p \delta_j z_j^2\\
\mathbb{E}T_R &= \text{tr}(\sigma^2 K_R^2) + \sigma^2_{\varepsilon} np = \sigma^2 \sum \delta_j^2 + \sigma^2_{\varepsilon}np\\
T_R^* &= \frac{T_R - \sigma^2_{\varepsilon}np}{\sigma^2_{\varepsilon}\sqrt{2\sum \delta_j^2}} ~\overset{H_0}{\dot\sim}~ \mathcal{N}(0, 1)\\
\mathbb{E}T_R^* &= \frac{\mathbb{E}T_R - \sigma_{\varepsilon}^2np}{\sigma^2_{\varepsilon}\sqrt{2\sum \delta_j^2}} = \frac{\sigma^2}{\sigma^2_{\varepsilon}}\sqrt{\frac{1}{2}\sum\delta_j^2}
\end{align}


#### Fixed Effects Test

Let $K_F = X(X^TX)^{-1}X^T$.

\begin{align}
T_F &= \text{tr}(K_F) = \sum_{j = 1}^p z_j^2\\
\mathbb{E}T_F &= \text{tr}(\sigma^2 K_F K_R) + \sigma^2_{\varepsilon} p = \sigma^2 np + \sigma^2_{\varepsilon}p\\
T_F^* &= \frac{T_F - \sigma^2_{\varepsilon}p}{\sigma^2_{\varepsilon}\sqrt{2p}} ~\overset{H_0}{\dot\sim}~ \mathcal{N}(0, 1)\\
\mathbb{E}T_F^* &= \frac{\mathbb{E}T_F - \sigma_{\varepsilon}^2p}{\sigma^2_{\varepsilon}\sqrt{2p}} = \frac{\sigma^2}{\sigma^2_{\varepsilon}}\cdot n\sqrt{\frac{p}{2}}
\end{align}

#### Ridge Kernel Test

Let $K_{\lambda} = X(X^TX + \lambda I_p)^{-1}X^T$.

\begin{align}
T_{\lambda} &= \text{tr}(K_{\lambda}) = \sum_{j = 1}^p \frac{\delta_j}{\delta_j + \lambda} z_j^2\\
\mathbb{E}T_{\lambda} &= \text{tr}(\sigma^2 K_{\lambda} K_R) + \sigma^2_{\varepsilon} \sum \frac{\delta_j}{\delta_j + \lambda}\\
T_{\lambda}^* &= \frac{T_{\lambda} - \sigma^2_{\varepsilon}\sum \frac{\delta_j}{\delta_j + \lambda}}{\sigma^2_{\varepsilon}\sqrt{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}} ~\overset{H_0}{\dot\sim}~ \mathcal{N}(0, 1)\\
\mathbb{E}T_{\lambda}^* &= \frac{\mathbb{E}T_{\lambda} - \sigma_{\varepsilon}^2 \sum \frac{\delta_j}{\delta_j + \lambda}}{\sigma^2_{\varepsilon}\sqrt{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}} = \frac{\sigma^2}{\sigma^2_{\varepsilon}}\cdot \frac{\text{tr}(K_{\lambda}K_R)}{\sqrt{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}}\\
\mathbb{E}T_{\lambda}^* &= \frac{\sigma^2}{\sigma^2_{\varepsilon}}\cdot \frac{\sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}}{\sqrt{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}}
\end{align}

(Trace identity for $\text{tr}(K_{\lambda}K_R)$ shown below.)

#### Asymptotic Equivalences

For $n$ sufficiently large, we can assume the expected values of the test statistics are

\begin{align*}
\mathbb{E}T_F^* &= \frac{\mathbb{E}T_F - \sigma_{\varepsilon}^2p}{\sigma^2_{\varepsilon}\sqrt{2p}} = \frac{\sigma^2}{\sigma^2_{\varepsilon}}\cdot n\sqrt{\frac{p}{2}}\\
\mathbb{E}T_R^* &= \frac{\mathbb{E}T_R - \sigma_{\varepsilon}^2np}{\sigma^2_{\varepsilon}\sqrt{2\sum \delta_j^2}} = \frac{\sigma^2}{\sigma^2_{\varepsilon}}\sqrt{\frac{1}{2}\sum\delta_j^2}\\
\mathbb{E}T_{\lambda}^* &= \frac{\sigma^2}{\sigma^2_{\varepsilon}}\cdot \frac{\sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}}{\sqrt{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}}
\end{align*}

Of course for $\lambda = 0$ we have $\mathbb{E}T_{\lambda}^* = \mathbb{E}T_F^*$ as usual.  Considering the limit as $\lambda \to \infty$ of the square of the second quotient in $T_{\lambda}^*$ (which is the only part that depends on $\lambda$):

\begin{align*}
\lim_{\lambda \to \infty} \frac{\left(\sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}\right)^2}{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}
&= \lim \frac{\frac{1}{1/\lambda}}{\frac{1}{1/\lambda}}\frac{\left(\sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}\right)^2}{2\sum \left(\frac{\delta_j}{\delta_j + \lambda}\right)^2}\\
&= \lim \frac{\left(\sum_{j = 1}^p \frac{\delta_j^2}{\delta_j/\lambda + 1}\right)^2}{2\sum \left(\frac{\delta_j}{\delta_j/\lambda + 1}\right)^2}\\
&= \frac{\left(\sum_{j = 1}^p \delta_j^2\right)^2}{2\sum \delta_j^2} = \left(\mathbb{E}T_{R}^*\right)^2\\
\Rightarrow \lim_{\lambda \to \infty} \mathbb{E}T_{\lambda}^* &= \mathbb{E}T_R^*
\end{align*}

Thus the expected value of the ridge test statistic $T_{\lambda}^*$ converges to that of the random effects test statistics $T_R^*$, and so the two tests have approximately equal power for sufficiently large $n$ and $\lambda$.

#### Computing $\text{tr}(K_RK_{\lambda})$

__Claim__ $\text{tr}(K_RK_{\lambda}) = \sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}$

__Proof__

We use the SVD $X = U D V^T$. We know that

\begin{align}
K_R &= XX^T = UDD^T U^T\\
K_{\lambda} &= UD(D^TD + \lambda I)^{-1}D^TU^T.
\end{align}

Plug these identities into the trace:

\begin{align}
\text{tr}(K_RK_{\lambda}) &= \text{tr}\left([UDD^TU^T][UD(D^TD + \lambda I)^{-1}D^TU^T]\right)\\
&= \text{tr}\left(D^TU^TUDD^TU^TUD(D^TD + \lambda I)^{-1}\right)\\
&= \text{tr}\left(D^TDD^TD(D^TD + \lambda I)^{-1}\right)\\
&= \sum_{j = 1}^p \frac{\delta_j^2}{\delta_j + \lambda}
\end{align}



\newpage

# References




## Appendix: Aim 2

### Geometric Structure of $\text{Sym}^+(q)$

This is a summary of results given in @schwartzman2006random and @zhu2009intrinsic.

$\text{Sym}^+(q)$ is a smooth Riemannian manifold of dimension $\frac{1}{2}q(q + 1)$
